{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append('../datasets')\n",
    "from datasets.manager import IMDBBinary, DD\n",
    "import torch \n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "#from utils.utils import visualise_graph, get_adjacency_and_features\n",
    "from utils.utils import get_adjacency_and_features, create_batch_from_loader\n",
    "#from src.gnn import GNNClassifier\n",
    "\n",
    "from datasets.dataset import *\n",
    "from train import Training\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import statistics\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from src.models import GCN, GAT, GraphDenseNet, GraphSAGE, GIN\n",
    "from datasets.dataloader import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"model_type\": \"GraphSAGE\",  # \"GCN\", \"GAT\", \"GIN\", \"GraphSAGE\"\n",
    "               \"n_graph_subsampling\": 0, # the number of running graph subsampling each train graph data run subsampling 5 times: increasing graph data 5 times\n",
    "               \"graph_node_subsampling\": True, # TRUE: removing node randomly to subsampling and augmentation of graph dataset \\n'+\n",
    "                # FALSE: removing edge randomly to subsampling and augmentation of graph dataset\n",
    "               \"graph_subsampling_rate\": 0.2, # graph subsampling rate\n",
    "               \"dataset\": \"DD\", \n",
    "               \"pooling_type\": \"mean\", \n",
    "               \"seed\": 42,\n",
    "               \"n_folds\": 10, \n",
    "               \"cuda\": True, \n",
    "               \"lr\": 0.001, \n",
    "               \"epochs\": 50, \n",
    "               \"weight_decay\":5e-4,\n",
    "               \"batch_size\": 32, \n",
    "               \"dropout\": 0, # dropout rate of layer\n",
    "               \"num_lay\": 5, \n",
    "               \"num_agg_layer\": 2, # the number of graph aggregation layers\n",
    "               \"hidden_agg_lay_size\": 64, # size of hidden graph aggregation layer\n",
    "               \"fc_hidden_size\": 128, # size of fully-connected layer after readout\n",
    "               \"threads\":10, # how many subprocesses to use for data loading\n",
    "               \"random_walk\":True,\n",
    "               \"walk_length\": 20, # walk length of random walk, \n",
    "               \"num_walk\": 10, # num of random walk\n",
    "               \"p\": 0.65, # Possibility to return to the previous vertex, how well you navigate around\n",
    "               \"q\": 0.35, # Possibility of moving away from the previous vertex, how well you are exploring new places\n",
    "               \"print_logger\": 10,  # printing rate\n",
    "               \"eps\":0.0, # for GIN only\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Train:\n",
    "    def __init__(self, params):\n",
    "        \"\"\"\n",
    "        Trainer class for training and evaluating a GNN model.\n",
    "\n",
    "        Args:\n",
    "            params (dict): Dictionary of training and model parameters.\n",
    "\n",
    "        Note:\n",
    "            - The dataset is automatically loaded depending on 'dataset' specified in params.\n",
    "            - Only supports 'IMDB' or 'DD' datasets as currently coded.\n",
    "        \"\"\"\n",
    "        if params[\"dataset\"] == \"IMDB\":\n",
    "            self.dataset = IMDBBinary()\n",
    "        elif params[\"dataset\"] == \"DD\":\n",
    "            self.dataset = DD()\n",
    "\n",
    "        self.params = params\n",
    "\n",
    "        # Extract the graph data and their corresponding labels\n",
    "        self.x_dataset, self.y_dataset = self.dataset.dataset.get_data(), self.dataset.dataset.get_targets()\n",
    "\n",
    "        # Select device (GPU if available and requested, else CPU)\n",
    "        if self.params[\"cuda\"] and torch.cuda.is_available():\n",
    "            self.device = \"cuda:0\"\n",
    "        else:\n",
    "            self.device = \"cpu\"\n",
    "\n",
    "        self.model = self.get_model()  # Instantiate the model\n",
    "        self.loss_fn = F.cross_entropy  # Loss function for classification tasks\n",
    "\n",
    "    def get_model(self):\n",
    "        \"\"\"\n",
    "        Instantiates the model specified in params[\"model_type\"].\n",
    "\n",
    "        Returns:\n",
    "            torch.nn.Module: The corresponding GNN model moved to the correct device.\n",
    "        \"\"\"\n",
    "        input_features = self.x_dataset[0].x.shape[1]  # Number of input node features\n",
    "\n",
    "        if self.params[\"model_type\"] == 'GCN':\n",
    "            model = GCN(\n",
    "                n_feat=input_features,\n",
    "                n_class=2,\n",
    "                n_layer=self.params['num_agg_layer'],\n",
    "                agg_hidden=self.params['hidden_agg_lay_size'],\n",
    "                fc_hidden=self.params['fc_hidden_size'],\n",
    "                dropout=self.params['dropout'],\n",
    "                pool_type=self.params['pooling_type'],\n",
    "                device=self.device\n",
    "            ).to(self.device)\n",
    "\n",
    "        elif self.params[\"model_type\"] == 'GAT':\n",
    "            model = GAT(\n",
    "                n_feat=input_features,\n",
    "                n_class=2,\n",
    "                n_layer=self.params['num_agg_layer'],\n",
    "                agg_hidden=self.params['hidden_agg_lay_size'],\n",
    "                fc_hidden=self.params['fc_hidden_size'],\n",
    "                dropout=self.params['dropout'],\n",
    "                pool_type=self.params['pooling_type'],\n",
    "                device=self.device\n",
    "            ).to(self.device)\n",
    "\n",
    "        elif self.params[\"model_type\"] == 'GraphSAGE':\n",
    "            model = GraphSAGE(\n",
    "                n_feat=input_features,\n",
    "                n_class=2,\n",
    "                n_layer=self.params['num_agg_layer'],\n",
    "                agg_hidden=self.params['hidden_agg_lay_size'],\n",
    "                fc_hidden=self.params['fc_hidden_size'],\n",
    "                dropout=self.params['dropout'],\n",
    "                pool_type=self.params[\"pooling_type\"],\n",
    "                device=self.device\n",
    "            ).to(self.device)\n",
    "\n",
    "        elif self.params[\"model_type\"] == 'GIN':\n",
    "            model = GIN(\n",
    "                n_feat=input_features,\n",
    "                n_class=2,\n",
    "                n_layer=self.params['num_agg_layer'],\n",
    "                agg_hidden=self.params['hidden_agg_lay_size'],\n",
    "                fc_hidden=self.params['fc_hidden_size'],\n",
    "                dropout=self.params['dropout'],\n",
    "                pool_type=self.params[\"pooling_type\"],\n",
    "                device=self.device\n",
    "            ).to(self.device)\n",
    "\n",
    "        return model\n",
    "\n",
    "    def loaders_train_test_setup(self):\n",
    "        \"\"\"\n",
    "        Sets up data loading, optimizer, and learning rate scheduler.\n",
    "\n",
    "        Returns:\n",
    "            Tuple: (DataLoader, optimizer, scheduler)\n",
    "        \"\"\"\n",
    "        # Create a custom DataLoader that simply returns indices (no batching)\n",
    "        loader = torch.utils.data.DataLoader(\n",
    "            range(len(self.x_dataset)),\n",
    "            batch_size=1,\n",
    "            shuffle=True,\n",
    "            num_workers=0,\n",
    "            pin_memory=True,\n",
    "            drop_last=False,\n",
    "            collate_fn=lambda x: x  # x will be a list of one index\n",
    "        )\n",
    "\n",
    "        # Count and display number of trainable parameters\n",
    "        c = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
    "        print('N trainable parameters:', c)\n",
    "\n",
    "        # Define Adam optimizer with weight decay\n",
    "        optimizer = optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.model.parameters()),\n",
    "            lr=self.params[\"lr\"],\n",
    "            weight_decay=self.params[\"weight_decay\"],\n",
    "            betas=(0.5, 0.999)\n",
    "        )\n",
    "\n",
    "        # Define learning rate scheduler (reduce LR at epochs 20 and 30)\n",
    "        scheduler = lr_scheduler.MultiStepLR(optimizer, [20, 30], gamma=0.1)\n",
    "\n",
    "        return loader, optimizer, scheduler\n",
    "\n",
    "    def train(self, train_loader, optimizer, scheduler, epoch):\n",
    "        \"\"\"\n",
    "        One training epoch over the dataset.\n",
    "\n",
    "        Args:\n",
    "            train_loader (DataLoader)\n",
    "            optimizer (Optimizer)\n",
    "            scheduler (Scheduler)\n",
    "            epoch (int): Current epoch index\n",
    "\n",
    "        Returns:\n",
    "            float: Average time per iteration\n",
    "        \"\"\"\n",
    "        self.model.train()\n",
    "        train_loss, n_samples = 0, 0\n",
    "        total_time_iter = 0\n",
    "        start = time.time()\n",
    "\n",
    "        for batch_idx, data_batch in enumerate(train_loader):\n",
    "            idx = data_batch[0]  # Extract index\n",
    "\n",
    "            x = self.x_dataset[idx]\n",
    "            y = self.y_dataset[idx]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if params[\"model_type\"] == \"GraphSAGE\":\n",
    "                # beware, GraphSAGE does not take adjency matrix as input\n",
    "                output = self.model(x) # Forward pass\n",
    "\n",
    "            else: \n",
    "                A, f = get_adjacency_and_features(x)\n",
    "                A = A.to(self.device)\n",
    "                f = f.to(self.device)\n",
    "                y = torch.tensor([y], device=self.device)\n",
    "\n",
    "                output = self.model(f, A) # Forward pass\n",
    "\n",
    "            y = torch.tensor([y], device=self.device)  # Wrap in tensor for batch dim\n",
    "\n",
    "        \n",
    "            loss = self.loss_fn(output.unsqueeze(0), y)  # Add batch dimension to output\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Timing and logging\n",
    "            time_iter = time.time() - start\n",
    "            total_time_iter += time_iter\n",
    "            train_loss += loss.item()\n",
    "            n_samples += 1\n",
    "\n",
    "            if batch_idx % self.params[\"print_logger\"] == 0 or batch_idx == len(train_loader) - 1:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} (avg: {:.6f}) \\tsec/iter: {:.4f}'.format(\n",
    "                    epoch, n_samples, len(train_loader.dataset),\n",
    "                    100. * (batch_idx + 1) / len(train_loader),\n",
    "                    loss.item(), train_loss / n_samples, time_iter / (batch_idx + 1)\n",
    "                ))\n",
    "\n",
    "            start = time.time()  # Reset timer\n",
    "\n",
    "        scheduler.step()  # Adjust learning rate\n",
    "        return total_time_iter / (len(train_loader) + 1)\n",
    "\n",
    "    def evaluate(self, test_loader):\n",
    "        \"\"\"\n",
    "        Evaluate model on the test set.\n",
    "\n",
    "        Args:\n",
    "            test_loader (DataLoader)\n",
    "\n",
    "        Returns:\n",
    "            float: Accuracy on the test set\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        correct, n_samples = 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch_idx, data_batch in enumerate(test_loader):\n",
    "                idx = data_batch[0]\n",
    "                x = self.x_dataset[idx]\n",
    "                y = self.y_dataset[idx]\n",
    "\n",
    "                if params[\"model_type\"] == \"GraphSAGE\":\n",
    "                    output = self.model(x)\n",
    "                else: \n",
    "                    A, f = get_adjacency_and_features(x)\n",
    "                    A = A.to(self.device)\n",
    "                    f = f.to(self.device)\n",
    "                    y = torch.tensor([y], device=self.device)\n",
    "\n",
    "                    output = self.model(f, A)\n",
    "\n",
    "                # Prediction: binary or multi-class\n",
    "                if output.shape[-1] == 1:\n",
    "                    pred = (torch.sigmoid(output) > 0.5).long()\n",
    "                else:\n",
    "                    pred = output.argmax(dim=-1)\n",
    "\n",
    "                correct += (pred == y).sum().item()\n",
    "                n_samples += 1\n",
    "\n",
    "        acc = 100. * correct / n_samples\n",
    "        print(f'Test set (epoch {self.params[\"epochs\"]}): Accuracy: {correct}/{n_samples} ({acc:.2f}%)\\n')\n",
    "\n",
    "        return acc\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Run the full training and evaluation loop.\n",
    "\n",
    "        Returns:\n",
    "            list: [dataset name, dataset name (again), best accuracy achieved]\n",
    "        \"\"\"\n",
    "        loader, optimizer, scheduler = self.loaders_train_test_setup()\n",
    "        total_time = 0\n",
    "        best_acc = 0\n",
    "        patience_counter = 0\n",
    "        patience = self.params.get(\"early_stopping_patience\", 5)\n",
    "\n",
    "        for epoch in tqdm(range(self.params[\"epochs\"]), desc=\"Epochs\", position=1, leave=False):\n",
    "            total_time_iter = self.train(loader, optimizer, scheduler, epoch)\n",
    "            total_time += total_time_iter\n",
    "            acc = self.evaluate(loader)  # Same loader used for train/test (no split)\n",
    "\n",
    "            # Early stopping logic\n",
    "            if acc > best_acc:\n",
    "                best_acc = acc\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch + 1} epochs.\")\n",
    "                break\n",
    "\n",
    "        print(f'Best Accuracy: {best_acc:.2f}%')\n",
    "        print(f'Average training time per epoch: {total_time / (epoch + 1):.2f} seconds')\n",
    "\n",
    "        return [self.params[\"dataset\"], self.params[\"dataset\"], best_acc]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDB = IMDBBinary()\n",
    "# DD = DD()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Train(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N trainable parameters: 36482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [1/1178 (0%)]\tLoss: 0.683738 (avg: 0.683738) \tsec/iter: 0.2101\n",
      "Train Epoch: 0 [11/1178 (1%)]\tLoss: 0.608161 (avg: 0.676393) \tsec/iter: 0.0006\n",
      "Train Epoch: 0 [21/1178 (2%)]\tLoss: 0.590466 (avg: 0.684501) \tsec/iter: 0.0002\n",
      "Train Epoch: 0 [31/1178 (3%)]\tLoss: 0.639477 (avg: 0.698243) \tsec/iter: 0.0001\n",
      "Train Epoch: 0 [41/1178 (3%)]\tLoss: 0.600413 (avg: 0.690745) \tsec/iter: 0.0006\n",
      "Train Epoch: 0 [51/1178 (4%)]\tLoss: 0.776364 (avg: 0.697600) \tsec/iter: 0.0001\n",
      "Train Epoch: 0 [61/1178 (5%)]\tLoss: 0.653939 (avg: 0.698377) \tsec/iter: 0.0001\n",
      "Train Epoch: 0 [71/1178 (6%)]\tLoss: 0.596028 (avg: 0.691859) \tsec/iter: 0.0002\n",
      "Train Epoch: 0 [81/1178 (7%)]\tLoss: 0.557506 (avg: 0.686922) \tsec/iter: 0.0001\n",
      "Train Epoch: 0 [91/1178 (8%)]\tLoss: 0.808094 (avg: 0.695079) \tsec/iter: 0.0001\n",
      "Train Epoch: 0 [101/1178 (9%)]\tLoss: 0.591872 (avg: 0.691841) \tsec/iter: 0.0001\n",
      "Train Epoch: 0 [111/1178 (9%)]\tLoss: 0.606068 (avg: 0.692875) \tsec/iter: 0.0000\n",
      "Train Epoch: 0 [121/1178 (10%)]\tLoss: 0.830264 (avg: 0.692067) \tsec/iter: 0.0001\n",
      "Train Epoch: 0 [131/1178 (11%)]\tLoss: 0.581223 (avg: 0.691111) \tsec/iter: 0.0000\n",
      "Train Epoch: 0 [141/1178 (12%)]\tLoss: 0.774389 (avg: 0.694568) \tsec/iter: 0.0026\n",
      "Train Epoch: 0 [151/1178 (13%)]\tLoss: 0.738292 (avg: 0.695390) \tsec/iter: 0.0000\n",
      "Train Epoch: 0 [161/1178 (14%)]\tLoss: 0.656511 (avg: 0.695029) \tsec/iter: 0.0000\n",
      "Train Epoch: 0 [171/1178 (15%)]\tLoss: 0.613168 (avg: 0.692813) \tsec/iter: 0.0000\n",
      "Train Epoch: 0 [181/1178 (15%)]\tLoss: 0.828826 (avg: 0.692420) \tsec/iter: 0.0000\n",
      "Train Epoch: 0 [191/1178 (16%)]\tLoss: 0.571159 (avg: 0.690458) \tsec/iter: 0.0000\n",
      "Train Epoch: 0 [201/1178 (17%)]\tLoss: 0.877473 (avg: 0.690051) \tsec/iter: 0.0001\n",
      "Train Epoch: 0 [211/1178 (18%)]\tLoss: 0.832329 (avg: 0.691050) \tsec/iter: 0.0001\n",
      "Train Epoch: 0 [221/1178 (19%)]\tLoss: 0.468273 (avg: 0.684034) \tsec/iter: 0.0000\n",
      "Train Epoch: 0 [231/1178 (20%)]\tLoss: 0.469071 (avg: 0.684534) \tsec/iter: 0.0000\n",
      "Train Epoch: 0 [241/1178 (20%)]\tLoss: 0.533147 (avg: 0.686354) \tsec/iter: 0.0000\n",
      "Train Epoch: 0 [251/1178 (21%)]\tLoss: 0.476268 (avg: 0.682774) \tsec/iter: 0.0000\n",
      "Train Epoch: 0 [261/1178 (22%)]\tLoss: 0.511379 (avg: 0.684365) \tsec/iter: 0.0000\n",
      "Train Epoch: 0 [271/1178 (23%)]\tLoss: 1.000168 (avg: 0.682722) \tsec/iter: 0.0000\n",
      "Train Epoch: 0 [281/1178 (24%)]\tLoss: 0.501112 (avg: 0.682670) \tsec/iter: 0.0000\n",
      "Train Epoch: 0 [291/1178 (25%)]\tLoss: 0.928835 (avg: 0.682566) \tsec/iter: 0.0000\n",
      "Train Epoch: 0 [301/1178 (26%)]\tLoss: 0.541509 (avg: 0.683760) \tsec/iter: 0.0000\n",
      "Train Epoch: 0 [311/1178 (26%)]\tLoss: 0.899605 (avg: 0.683518) \tsec/iter: 0.0001\n",
      "Train Epoch: 0 [321/1178 (27%)]\tLoss: 0.531022 (avg: 0.683233) \tsec/iter: 0.0001\n",
      "Train Epoch: 0 [331/1178 (28%)]\tLoss: 0.544508 (avg: 0.684139) \tsec/iter: 0.0000\n",
      "Train Epoch: 0 [341/1178 (29%)]\tLoss: 0.525845 (avg: 0.683108) \tsec/iter: 0.0000\n",
      "Train Epoch: 0 [351/1178 (30%)]\tLoss: 0.524645 (avg: 0.681818) \tsec/iter: 0.0000\n",
      "Train Epoch: 0 [361/1178 (31%)]\tLoss: 0.503277 (avg: 0.681786) \tsec/iter: 0.0000\n",
      "Train Epoch: 0 [371/1178 (31%)]\tLoss: 0.533707 (avg: 0.682662) \tsec/iter: 0.0000\n",
      "Train Epoch: 0 [381/1178 (32%)]\tLoss: 0.827715 (avg: 0.684836) \tsec/iter: 0.0000\n",
      "Train Epoch: 0 [391/1178 (33%)]\tLoss: 0.553623 (avg: 0.683455) \tsec/iter: 0.0001\n",
      "Train Epoch: 0 [401/1178 (34%)]\tLoss: 0.845343 (avg: 0.684105) \tsec/iter: 0.0000\n",
      "Train Epoch: 0 [411/1178 (35%)]\tLoss: 0.538220 (avg: 0.682482) \tsec/iter: 0.0000\n",
      "Train Epoch: 0 [421/1178 (36%)]\tLoss: 0.487221 (avg: 0.680320) \tsec/iter: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [431/1178 (37%)]\tLoss: 0.542644 (avg: 0.683183) \tsec/iter: 0.0001\n",
      "Train Epoch: 0 [441/1178 (37%)]\tLoss: 0.833045 (avg: 0.684368) \tsec/iter: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 247\u001b[0m, in \u001b[0;36mTrain.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    244\u001b[0m patience \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mearly_stopping_patience\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m\"\u001b[39m, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 247\u001b[0m     total_time_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m     total_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m total_time_iter\n\u001b[1;32m    249\u001b[0m     acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(loader)  \u001b[38;5;66;03m# Same loader used for train/test (no split)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[3], line 154\u001b[0m, in \u001b[0;36mTrain.train\u001b[0;34m(self, train_loader, optimizer, scheduler, epoch)\u001b[0m\n\u001b[1;32m    150\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraphSAGE\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# beware, GraphSAGE does not take adjency matrix as input\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \n\u001b[1;32m    157\u001b[0m     A, f \u001b[38;5;241m=\u001b[39m get_adjacency_and_features(x)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/MVA_Graphical_Models/src/models.py:132\u001b[0m, in \u001b[0;36mGraphSAGE.forward\u001b[0;34m(self, x_graph_data, batch)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layer):\n\u001b[1;32m    131\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m--> 132\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraphsage_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    133\u001b[0m     x_layers\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Concaténation des sorties des couches\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch_geometric/nn/conv/sage_conv.py:139\u001b[0m, in \u001b[0;36mSAGEConv.forward\u001b[0;34m(self, x, edge_index, size)\u001b[0m\n\u001b[1;32m    137\u001b[0m x_r \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_weight \u001b[38;5;129;01mand\u001b[39;00m x_r \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 139\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlin_r\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_r\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize:\n\u001b[1;32m    142\u001b[0m     out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mnormalize(out, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2.\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m x_data \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mx_dataset\n\u001b[1;32m      2\u001b[0m x0 \u001b[38;5;241m=\u001b[39m x_data[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mmodel\n",
      "\u001b[0;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "x_data = trainer.x_dataset\n",
    "x0 = x_data[0]\n",
    "model = trainer.model\n",
    "edge_index = x0.edge_index - 1\n",
    "x = x0.x\n",
    "print(x.shape)\n",
    "print(edge_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avant modèle :\n",
      "edge_index min : 0\n",
      "edge_index max : 326\n",
      "x.shape[0] : 327\n"
     ]
    }
   ],
   "source": [
    "print(\"Avant modèle :\")\n",
    "print(\"edge_index min :\", edge_index.min().item())\n",
    "print(\"edge_index max :\", edge_index.max().item())\n",
    "print(\"x.shape[0] :\", x.shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nail_env)",
   "language": "python",
   "name": "nail_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
