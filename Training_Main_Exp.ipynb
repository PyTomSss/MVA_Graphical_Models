{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Training Graph Neural Networks with a Custom `ModelAssessment` Class**\n",
    "\n",
    "This notebook provides a detailed walkthrough of a custom-built `Train` class for training **Graph Neural Networks (GNNs)** using PyTorch.  \n",
    "It supports multiple architectures—**GCN**, **GAT**, **GraphSAGE**, and **GIN**—and includes a full training pipeline with evaluation and **early stopping**.\n",
    "\n",
    "---\n",
    "\n",
    "### Setup and Requirements\n",
    "\n",
    "Make sure the following packages and custom modules are installed:\n",
    "\n",
    "```bash\n",
    "pip install torch torchvision torchaudio tqdm\n",
    "```\n",
    "\n",
    "You will also need the following **custom modules** available in your working directory:\n",
    "\n",
    "- `datasets/` (e.g., `IMDB`, `DD`, `PROTEINS`)\n",
    "- `src/models.py` (GNN architectures)\n",
    "- `utils/utils.py` (helper functions)\n",
    "\n",
    "---\n",
    "\n",
    "### What the `ModelAssessment` Class Does\n",
    "\n",
    "The `Train` class handles the **entire training lifecycle**, including:\n",
    "\n",
    "- **Dataset loading** (e.g., IMDB or DD)\n",
    "- **Model initialization** (GCN, GAT, GraphSAGE, GIN)\n",
    "- **Optional graph subsampling and augmentation**\n",
    "- **Training loop** with optimizer & scheduler\n",
    "- **Evaluation** with accuracy tracking and early stopping\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../datasets')\n",
    "from datasets.manager import IMDBBinary, DD, PROTEINS\n",
    "from datasets.dataset import *\n",
    "from Pipeline.Model_Assessment import ModelAssessment, plot_gnn_comparison\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training Setup**\n",
    "\n",
    "To understand the full training process used for benchmarking the different GNN architectures, please refer to the notebook *toy_examples.ipynb*, which outlines each step in detail, along with the properties of the core classes.\n",
    "\n",
    "We begin this benchmark by defining the list of hyperparameters that will be used to generate the **search grid** for model selection.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"model_type\": \"GCN\",  # \"GCN\", \"GAT\", \"GIN\", \"GraphSAGE\"\n",
    "               \"n_graph_subsampling\": 0, # the number of running graph subsampling each train graph data run subsampling 5 times: increasing graph data 5 times\n",
    "               \"graph_node_subsampling\": True, # TRUE: removing node randomly to subsampling and augmentation of graph dataset \\n'+\n",
    "                # FALSE: removing edge randomly to subsampling and augmentation of graph dataset\n",
    "               \"graph_subsampling_rate\": 0.2, # graph subsampling rate\n",
    "               \"dataset\": \"DD\", \n",
    "               \"pooling_type\": \"mean\", \n",
    "               \"seed\": 42,\n",
    "               \"n_folds\": 10, \n",
    "               \"cuda\": True, \n",
    "               \"lr\": [0.001, 0.01, 0.1], \n",
    "               \"epochs\": 50, \n",
    "               \"weight_decay\":5e-4,\n",
    "               \"batch_size\": 32, \n",
    "               \"dropout\": 0, # dropout rate of layer\n",
    "               \"num_lay\": [2, 3, 5], \n",
    "               \"num_agg_layer\": 2, # the number of graph aggregation layers\n",
    "               \"hidden_agg_lay_size\": [16, 32, 64], # size of hidden graph aggregation layer\n",
    "               \"fc_hidden_size\": 128, # size of fully-connected layer after readout\n",
    "               \"threads\":10, # how many subprocesses to use for data loading\n",
    "               \"random_walk\":True,\n",
    "               \"walk_length\": 20, # walk length of random walk, \n",
    "               \"num_walk\": 10, # num of random walk\n",
    "               \"p\": 0.65, # Possibility to return to the previous vertex, how well you navigate around\n",
    "               \"q\": 0.35, # Possibility of moving away from the previous vertex, how well you are exploring new places\n",
    "               \"print_logger\": 10,  # printing rate\n",
    "               \"eps\":0.0, # for GIN only\n",
    "               \"early_stopping\": False # early stopping\n",
    "               }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Benchmarking GNNs**\n",
    "\n",
    "First, we initialize a dictionary to store the performance of each GNN architecture across the different datasets.  \n",
    "Performance is measured using the **accuracy metric**, averaged and accompanied by its standard deviation across the different folds of a cross-validation procedure.\n",
    "\n",
    "This notebook trains models on **three datasets**:\n",
    "\n",
    "- **D&D**: A molecular graph dataset with 2 classes.\n",
    "- **PROTEINS**: Another molecular graph dataset with 2 classes.\n",
    "- **IMDB-BINARY**: A social network dataset used for binary graph classification.\n",
    "\n",
    "We conduct several experiments to evaluate the effect of various training and model selection techniques. The benchmark begins with a **standard evaluation setup** based on the experimental framework from [Federico et al., 2022](https://arxiv.org/abs/1912.09893).\n",
    "\n",
    "We then compare the results of each method across the datasets under **three different experimental settings**:\n",
    "\n",
    "- **With and without Early Stopping** during training.\n",
    "- **With and without Node Degree features** (i.e., including the node degree as an additional feature).\n",
    "- Using either **Grid Search** or **Random Search** during hyperparameter tuning.\n",
    "\n",
    "The results of these experiments will be visualized and analyzed to draw insights into the performance of different architectures and the impact of these common training strategies.\n",
    "\n",
    "We begin by initializing the results dictionary below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"benchmark\", \"random\", \"early\", \"node_features\"]\n",
    "datasets = [\"DD\", \"IMDB\", \"PROTEINS\"]\n",
    "\n",
    "acc = {metric: {ds: [] for ds in datasets} for metric in metrics}\n",
    "std_acc = {metric: {ds: [] for ds in datasets} for metric in metrics}\n",
    "\n",
    "# Exemple d'accès :\n",
    "# acc[\"benchmark\"][\"DD\"].append(0.81)\n",
    "# std_acc[\"early\"][\"PROTEINS\"].append(0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Selection**\n",
    "\n",
    "As explained in *toy_examples.ipynb*, we compare the following GNN architectures:\n",
    "\n",
    "- **GCN**: Graph Convolutional Network  \n",
    "- **GAT**: Graph Attention Network  \n",
    "- **GraphSAGE**: Neighborhood aggregation-based GNN  \n",
    "- **GIN**: Graph Isomorphism Network\n",
    "\n",
    "Each model is trained with a specific set of core hyperparameters, including:\n",
    "\n",
    "- Number of convolutional layers  \n",
    "- Embedding dimension  \n",
    "- Learning rate  \n",
    "\n",
    "---\n",
    "\n",
    "### **GNN Performance Comparison on 3 Datasets for Graph Classification**\n",
    "\n",
    "We now begin the training phase by initializing the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Intialisation des Datasets\n",
    "DD = DD()\n",
    "IMDB = IMDBBinary()\n",
    "PROTEINS = PROTEINS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training — All Architectures on All Datasets\n",
    "\n",
    "In this section, we loop through all selected **datasets** (`DD`, `IMDB`, `PROTEINS`) and **model architectures** (`GCN`, `GAT`, `GIN`, `GraphSAGE`, `Baseline`) to train and evaluate each combination.\n",
    "\n",
    "The training is performed using **grid search** for hyperparameter selection, and the resulting mean accuracy and standard deviation are recorded for each run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of datasets\n",
    "datasets = {\n",
    "    \"DD\": DD,\n",
    "    \"IMDB\": IMDB,\n",
    "    \"PROTEINS\": PROTEINS \n",
    "}\n",
    "\n",
    "# List of model types to train & Evaluate\n",
    "model_types = [\"GCN\", \"GAT\", \"GIN\", \"GraphSAGE\", \"Baseline\"]\n",
    "\n",
    "for dataset_name, dataset_obj in datasets.items():\n",
    "    print(f\"\\n Training on {dataset_name} dataset\")\n",
    "\n",
    "    for model in model_types:\n",
    "        print(f\"  ➤ Model: {model}\")\n",
    "        \n",
    "        params_list = params.copy()\n",
    "        params_list[\"model_type\"] = model\n",
    "        \n",
    "        # Initialize and run ModelAssessment\n",
    "        model_assessment = ModelAssessment(dataset_obj, params_list, random_search=False)\n",
    "        mean, std = model_assessment.assess()\n",
    "        \n",
    "        # Stockage des Resultats\n",
    "        acc[\"benchmark\"][dataset_name].append(mean)\n",
    "        std_acc[\"benchmark\"][dataset_name].append(std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Experiment 1: Evaluating GNN Performance With and Without Early Stopping**\n",
    "\n",
    "In this experiment, we compare the performance of Graph Neural Networks (GNNs) when trained **with** and **without early stopping**.\n",
    "\n",
    "**Early stopping** is a regularization technique that halts training when the model’s performance on a validation set no longer improves, helping to prevent overfitting and reduce unnecessary computation.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update param_list -> early stopping\n",
    "params[\"early_stopping\"] = True\n",
    "\n",
    "datasets = {\n",
    "    \"DD\": DD,\n",
    "    \"IMDB\": IMDB,\n",
    "    \"PROTEINS\": PROTEINS \n",
    "}\n",
    "\n",
    "# List of model types to train & Evaluate\n",
    "model_types = [\"GCN\", \"GAT\", \"GIN\", \"GraphSAGE\", \"Baseline\"]\n",
    "\n",
    "for dataset_name, dataset_obj in datasets.items():\n",
    "    print(f\"\\n Training on {dataset_name} dataset\")\n",
    "\n",
    "    for model in model_types:\n",
    "        print(f\"  ➤ Model: {model}\")\n",
    "        \n",
    "        params_list = params.copy()\n",
    "        params_list[\"model_type\"] = model\n",
    "        \n",
    "        # Initialize and run ModelAssessment\n",
    "        model_assessment = ModelAssessment(dataset_obj, params_list, random_search=False)\n",
    "        mean, std = model_assessment.assess()\n",
    "        \n",
    "        # Stockage des Resultats -> pour early stopping\n",
    "        acc[\"early\"][dataset_name].append(mean)\n",
    "        std_acc[\"early\"][dataset_name].append(std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Experiment 2: Evaluating GNN Performance With and Without Node Degree Features**\n",
    "\n",
    "In this experiment, we evaluate the impact of adding **node degree information** to the feature matrices of the graphs.\n",
    "\n",
    "We compare the performance of each GNN architecture when trained:\n",
    "- **With node degrees** included as an additional feature per node.\n",
    "- **Without node degrees**, using only the original node features.\n",
    "\n",
    "This allows us to assess whether incorporating structural information like node connectivity can improve model accuracy across datasets.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update param_list -> Node Degree\n",
    "params[\"Node_Degree\"] = True\n",
    "\n",
    "datasets = {\n",
    "    \"DD\": DD,\n",
    "    \"IMDB\": IMDB,\n",
    "    \"PROTEINS\": PROTEINS \n",
    "}\n",
    "\n",
    "# List of model types to train & Evaluate\n",
    "model_types = [\"GCN\", \"GAT\", \"GIN\", \"GraphSAGE\", \"Baseline\"]\n",
    "\n",
    "for dataset_name, dataset_obj in datasets.items():\n",
    "    print(f\"\\n Training on {dataset_name} dataset\")\n",
    "\n",
    "    for model in model_types:\n",
    "        print(f\"  ➤ Model: {model}\")\n",
    "        \n",
    "        params_list = params.copy()\n",
    "        params_list[\"model_type\"] = model\n",
    "        \n",
    "        # Initialize and run ModelAssessment\n",
    "        model_assessment = ModelAssessment(dataset_obj, params_list, random_search=False)\n",
    "        mean, std = model_assessment.assess()\n",
    "        \n",
    "        # Stockage des Resultats -> pour node degree\n",
    "        acc[\"node_features\"][dataset_name].append(mean)\n",
    "        std_acc[\"node_features\"][dataset_name].append(std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Experiment 3: Comparing Grid Search and Random Search for Hyperparameter Tuning**\n",
    "\n",
    "In this experiment, we compare two strategies for **hyperparameter tuning** during model selection:\n",
    "\n",
    "- **Grid Search**: systematically explores all combinations of specified hyperparameter values.\n",
    "- **Random Search**: samples a subset of random combinations from the hyperparameter space.\n",
    "\n",
    "This comparison allows us to evaluate the trade-off between **exploration efficiency** and **computational cost**, and to assess whether Random Search can achieve comparable performance with fewer evaluations.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Update param_list -> random search\n",
    "\n",
    "datasets = {\n",
    "    \"DD\": DD,\n",
    "    \"IMDB\": IMDB,\n",
    "    \"PROTEINS\": PROTEINS \n",
    "}\n",
    "\n",
    "# List of model types to train & Evaluate\n",
    "model_types = [\"GCN\", \"GAT\", \"GIN\", \"GraphSAGE\", \"Baseline\"]\n",
    "\n",
    "for dataset_name, dataset_obj in datasets.items():\n",
    "    print(f\"\\n Training on {dataset_name} dataset\")\n",
    "\n",
    "    for model in model_types:\n",
    "        print(f\"  ➤ Model: {model}\")\n",
    "        \n",
    "        params_list = params.copy()\n",
    "        params_list[\"model_type\"] = model\n",
    "        \n",
    "        # Initialize and run ModelAssessment\n",
    "        model_assessment = ModelAssessment(dataset_obj, params_list, random_search=True) ## -> On teste avec RandomSearch\n",
    "        mean, std = model_assessment.assess()\n",
    "        \n",
    "        # Stockage des Resultats \n",
    "        acc[\"random\"][dataset_name].append(mean)\n",
    "        std_acc[\"random\"][dataset_name].append(std)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Visualizing Experimental Results with Boxplots**\n",
    "\n",
    "In this section, we visualize the outcomes of our different experiments using **boxplots**.  \n",
    "These plots illustrate the **performance distribution** (in terms of accuracy) of each GNN architecture across the **three datasets** (`D&D`, `PROTEINS`, `IMDB-BINARY`) under the various **experimental settings**:\n",
    "\n",
    "- With and without **Early Stopping**\n",
    "- With and without **Node Degree Features**\n",
    "- Using **Grid Search** vs. **Random Search** for model selection\n",
    "\n",
    "Boxplots provide a clear comparison of the **variance**, **robustness**, and **average accuracy** of each method, allowing us to draw conclusions on which settings contribute most to model performance and generalization.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"GCN\", \"GAT\", \"GIN\", \"GraphSAGE\", \"Baseline\"]\n",
    "\n",
    "## Plotting the results -> Experiment 1\n",
    "plot_gnn_comparison(models, datasets, acc[\"benchmark\"], std_acc[\"benchmark\"], acc[\"early\"], std_acc[\"early\"], \"Comparative Study of GNNs - Impact of Early Stopping on Training\", \"Without Early Stopping\", \"With Early Stopping\")\n",
    "\n",
    "## Plotting the results -> Experiment 2\n",
    "plot_gnn_comparison(models, datasets, acc[\"benchmark\"], std_acc[\"benchmark\"], acc[\"node_features\"], std_acc[\"node_features\"], \"Comparative Study of GNNs - Impact of Node Features on Training\", \"Without Node Features\", \"With Node Features\")\n",
    "\n",
    "## Plotting the results -> Experiment 3\n",
    "plot_gnn_comparison(models, datasets, acc[\"benchmark\"], std_acc[\"benchmark\"], acc[\"random\"], std_acc[\"random\"], \"Comparative Study of GNNs - Impact of Random Search on Training\", \"Without Random Search\", \"With Random Search\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
