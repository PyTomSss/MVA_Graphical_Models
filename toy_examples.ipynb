{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook de test des GNNs sur des Toys Examples : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tomrossa/Documents/MVA/MVA S2/Graphical Models/MVA_Graphical_Models/my_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append('../datasets')\n",
    "from datasets.manager import IMDBBinary, DD\n",
    "import torch \n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "#from utils.utils import visualise_graph, get_adjacency_and_features\n",
    "from utils.utils import get_adjacency_and_features\n",
    "#from src.gnn import GNNClassifier\n",
    "\n",
    "from datasets.dataset import *\n",
    "from train import Training\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDB = IMDBBinary()\n",
    "DD = DD()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Visualisation du dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_IMBD, y_IMDB = IMDB.dataset.get_data(), IMDB.dataset.get_targets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GraphData(x=torch.Size([18, 1]), edge_index=torch.Size([2, 153]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node8 = x_IMBD[8]\n",
    "node8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`GraphData(x=torch.Size([18, 1]), edge_index=torch.Size([2, 153]))` refers to a graph that has: \n",
    "- 18 nodes, each having features of dimension 1\n",
    "- 153 edges (each between 2 nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'visualise_graph' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvisualise_graph\u001b[49m(\u001b[38;5;241m8\u001b[39m, x_IMBD)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'visualise_graph' is not defined"
     ]
    }
   ],
   "source": [
    "visualise_graph(8, x_IMBD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The adjency matrix of such a graph is computed by `get_adjacency_and_features` and given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'node8' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m adjency_mat, feature_vect \u001b[38;5;241m=\u001b[39m get_adjacency_and_features(\u001b[43mnode8\u001b[49m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAdjency matrix:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(adjency_mat)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'node8' is not defined"
     ]
    }
   ],
   "source": [
    "adjency_mat, feature_vect = get_adjacency_and_features(node8)\n",
    "print(\"Adjency matrix:\\n\")\n",
    "print(adjency_mat)\n",
    "print(\"Feature vector: \\n\")\n",
    "print(feature_vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implémentation de la cross-validation ; \n",
    "\n",
    "Les hyperparamètres: \n",
    "- Learning Rate\n",
    "- Nombre de Couche de Convolution\n",
    "- Dimension de l'espace d'embedding\n",
    "- Critère d'Early Stopping\n",
    "- dropout / Weight Decay\n",
    "- Batch Size\n",
    "- Nombre d'Epochs\n",
    "\n",
    "Question de mette ou non la feature du degré des noeuds pour voir l'impact sur la performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class grid: \n",
    "    \n",
    "    def __init__(self, params_dict): \n",
    "        self.params_dict = params_dict\n",
    "\n",
    "    def get_combinations(self):\n",
    "        keys, values = zip(*self.param_dict.items())\n",
    "        return [dict(zip(keys, v)) for v in itertools.product(*values)]\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"HyperparameterGrid({self.param_dict})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelSelection(): \n",
    "    \n",
    "    def __init__(self, DATASET, data_split, model_class, grid): \n",
    "        self.model_class = model_class\n",
    "        self.grid = grid\n",
    "        self.DATASET = DATASET\n",
    "        self.train_split = GraphDatasetSubset(self.DATASET.dataset.get_data(), data_split[\"train\"])\n",
    "        self.validation_split = GraphDatasetSubset(self.DATASET.dataset.get_data(), data_split[\"validation\"])\n",
    "        self.best_config = None\n",
    "\n",
    "    def fit(self):\n",
    "        for config in self.grid.get_combinations():\n",
    "            print(f\"Training with configuration: {config}\")\n",
    "            \n",
    "            model = self.model_class(**config)\n",
    "            model.fit(self.train_split)\n",
    "            accuracy = model.evaluate(self.validation_split)\n",
    "\n",
    "            print(f\"Validation accuracy: {accuracy}\")\n",
    "\n",
    "            if self.best_config is None or accuracy > self.best_config[\"accuracy\"]:\n",
    "                self.best_config = {\"config\": config, \"accuracy\": accuracy}\n",
    "        \n",
    "    def get_best_config(self):\n",
    "        if self.best_config is None:\n",
    "            raise ValueError(\"No configurations have been evaluated.\")\n",
    "        return self.best_config[\"config\"], self.best_config[\"accuracy\"]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Holdout(): \n",
    "    def __init__(self, DATASET, train_split, train_size=0.9): \n",
    "        self.DATASET = DATASET\n",
    "        self.train_split = train_split\n",
    "        self.train_size = train_size\n",
    "        self.num_samples = len(self.train_split)\n",
    "        self.train_indices = None\n",
    "\n",
    "    def split(self):\n",
    "\n",
    "        indices = np.arange(self.num_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        self.train_indices = indices[:int(self.num_samples * self.train_size)]\n",
    "       \n",
    "\n",
    "    def get_splits(self):\n",
    "        if self.train_indices is None :\n",
    "            raise ValueError(\"Data has not been split yet.\")\n",
    "        return self.train_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelAssessment(): \n",
    "    \n",
    "    def __init__(self, model_class, DATASET, grid, Kfold=10, holdout=3): \n",
    "        self.model_class = model_class\n",
    "        self.DATASET = DATASET\n",
    "        self.grid = grid\n",
    "        self.Kfold = Kfold\n",
    "        self.best_configs = []\n",
    "        self.splits = DATASET.splits\n",
    "        self.holdout = holdout\n",
    "        self.fold_accuracy = []\n",
    "\n",
    "    def assess(self):\n",
    "\n",
    "        for i in tqdm(range(self.Kfold)):\n",
    "            train_data_idxs = self.splits[i][\"model_selection\"][0]\n",
    "            test_data_idxs = self.splits[i][\"test\"]\n",
    "            test_data = GraphDatasetSubset(self.DATASET.dataset.get_data(), test_data_idxs)\n",
    "\n",
    "            model_selection = ModelSelection(self.DATASET, train_data_idxs, self.model_class, self.grid)\n",
    "            model_selection.fit()\n",
    "            best_config, _ = model_selection.get_best_config()\n",
    "            self.best_configs.append(best_config)\n",
    "            inner_accuracy = []\n",
    "\n",
    "            for j in range(self.holdout): \n",
    "                \n",
    "                train_holdout = Holdout(self.DATASET, train_data_idxs)\n",
    "                train_holdout.split()\n",
    "                train_indices = train_holdout.get_splits()\n",
    "                \n",
    "                train_data = GraphDatasetSubset(self.DATASET.dataset.get_data(), train_indices)\n",
    "                model = self.model_class(**best_config)\n",
    "                model.fit(train_data)\n",
    "                accuracy = model.evaluate(test_data)\n",
    "                inner_accuracy.append(accuracy)\n",
    "            \n",
    "            self.fold_accuracy.append(np.mean(inner_accuracy))\n",
    "        \n",
    "        return np.mean(self.fold_accuracy), np.std(self.fold_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = IMDB.splits[0][\"model_selection\"][0]\n",
    "train_data = GraphDatasetSubset(IMDB.dataset.get_data(), idxs[\"train\"])\n",
    "val_data = GraphDatasetSubset(IMDB.dataset.get_data(), idxs[\"validation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On tente avec une configuration random pour voir si l'entraînement se lance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_list = {\"model_type\": \"GCN\",  \n",
    "               \"n_graph_subsampling\": 0, # the number of running graph subsampling each train graph data run subsampling 5 times: increasing graph data 5 times\n",
    "               \"graph_node_subsampling\": True, # TRUE: removing node randomly to subsampling and augmentation of graph dataset \\n'+\n",
    "                # FALSE: removing edge randomly to subsampling and augmentation of graph dataset\n",
    "               \"graph_subsampling_rate\": 0.2, # graph subsampling rate\n",
    "               \"dataset\": \"IMDB\", \n",
    "               \"pooling_type\": \"mean\", \n",
    "               \"seed\": 42,\n",
    "               \"n_folds\": 10, \n",
    "               \"cuda\": True, \n",
    "               \"lr\": 0.001, \n",
    "               \"epochs\": 50, \n",
    "               \"weight_decay\":5e-4,\n",
    "               \"batch_size\": 32, \n",
    "               \"dropout\": 0, # dropout rate of layer\n",
    "               \"num_lay\": 5, \n",
    "               \"num_agg_layer\": 2, # the number of graph aggregation layers\n",
    "               \"hidden_agg_lay_size\": 64, # size of hidden graph aggregation layer\n",
    "               \"fc_hidden_size\": 128, # size of fully-connected layer after readout\n",
    "               \"threads\":10, # how many subprocesses to use for data loading\n",
    "               \"random_walk\":True,\n",
    "               \"walk_length\": 20, # walk length of random walk, \n",
    "               \"num_walk\": 10, # num of random walk\n",
    "               \"p\": 0.65, # Possibility to return to the previous vertex, how well you navigate around\n",
    "               \"q\": 0.35, # Possibility of moving away from the previous vertex, how well you are exploring new places\n",
    "               \"print_logger\": 10  # printing rate\n",
    "               }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjency matrix:\n",
      "\n",
      "tensor([[0., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "Feature vector: \n",
      "\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "dataaaa = GraphDatasetSubset(DD.dataset.get_data(), idxs[\"train\"])\n",
    "adj, features = get_adjacency_and_features(dataaaa[0])\n",
    "print(\"Adjency matrix:\\n\")\n",
    "print(adj)\n",
    "print(\"Feature vector: \\n\")\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import statistics\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "from src.models import GCN, GAT, GraphDenseNet\n",
    "from datasets.dataloader import DataLoader\n",
    "\n",
    "\n",
    "class Train:\n",
    "    def __init__(self, params, dataset):\n",
    "      self.params = params \n",
    "      self.dataset = dataset\n",
    "\n",
    "      if self.params[\"cuda\"] and torch.cuda.is_available():\n",
    "          self.device = \"cuda:0\"\n",
    "      else: \n",
    "          self.device = \"cpu\"\n",
    "\n",
    "      self.model = self.get_model()\n",
    "      self.loss_fn = F.cross_entropy \n",
    "\n",
    "    \n",
    "    def get_model(self):\n",
    "        \n",
    "        adjacency, features = get_adjacency_and_features(self.dataset[0])\n",
    "        print(len(features))\n",
    "      \n",
    "        if self.params[\"model_type\"] == 'GCN':\n",
    "          \n",
    "            model = GCN(n_feat=89,\n",
    "                    n_class=2,\n",
    "                    n_layer=self.params['num_agg_layer'],\n",
    "                    agg_hidden=self.params['hidden_agg_lay_size'],\n",
    "                    fc_hidden=self.params['fc_hidden_size'],\n",
    "                    dropout=self.params['dropout'],\n",
    "                    pool_type=self.params['pooling_type'],\n",
    "                    device=self.device).to(self.device)\n",
    "            \n",
    "        elif self.params[\"model_type\"] == 'GAT':\n",
    "            \n",
    "            model = GAT(n_feat=89,\n",
    "                    n_class=2,\n",
    "                    n_layer=self.params['num_agg_layer'],\n",
    "                    agg_hidden=self.params['hidden_agg_lay_size'],\n",
    "                    fc_hidden=self.params['fc_hidden_size'],\n",
    "                    dropout=self.params['dropout'],\n",
    "                    pool_type=self.params['pooling_type'],\n",
    "                    device=self.device).to(self.device)\n",
    "            \n",
    "        elif self.params[\"model_type\"] == 'GraphDenseNet':\n",
    "            \n",
    "            model = GraphDenseNet(n_feat=89,\n",
    "                    n_class=2,\n",
    "                    n_layer=self.params['num_agg_layer'],\n",
    "                    agg_hidden=self.params['hidden_agg_lay_size'],\n",
    "                    fc_hidden=self.params['fc_hidden_size'],\n",
    "                    dropout=self.params['dropout'],\n",
    "                    pool_type=self.params[\"pooling_type\"],\n",
    "                    device=self.device).to(self.device)\n",
    "            \n",
    "        return model\n",
    "\n",
    "\n",
    "    def loaders_train_test_setup(self):\n",
    "\n",
    "        loader = DataLoader(self.dataset,\n",
    "                            batch_size=self.params[\"batch_size\"],\n",
    "                            shuffle=True,\n",
    "                            num_workers=0,#self.params[\"threads\"],\n",
    "                            pin_memory=True,\n",
    "                            drop_last=False)\n",
    "      \n",
    "      # Total trainable param\n",
    "        c = 0\n",
    "        for p in filter(lambda p: p.requires_grad, self.model.parameters()):\n",
    "            c += p.numel()\n",
    "        print('N trainable parameters:', c)\n",
    "\n",
    "        optimizer = optim.Adam(\n",
    "                        filter(lambda p: p.requires_grad, self.model.parameters()),\n",
    "                        lr=self.params[\"lr\"],\n",
    "                        weight_decay=self.params[\"weight_decay\"],\n",
    "                        betas=(0.5, 0.999))\n",
    "    \n",
    "        scheduler = lr_scheduler.MultiStepLR(optimizer, [20, 30], gamma=0.1)\n",
    "            \n",
    "        return loader, optimizer, scheduler\n",
    "\n",
    "    def train(self, train_loader, optimizer, scheduler, epoch):\n",
    "\n",
    "      total_time_iter = 0\n",
    "      self.model.train()\n",
    "      start = time.time()\n",
    "      train_loss, n_samples = 0, 0\n",
    "      for batch_idx, data in enumerate(train_loader):\n",
    "          optimizer.zero_grad()\n",
    "          output = self.model(data)\n",
    "          loss = self.loss_fn(output, data[4])\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          time_iter = time.time() - start\n",
    "          total_time_iter += time_iter\n",
    "          train_loss += loss.item() * len(output)\n",
    "          n_samples += len(output)\n",
    "          if batch_idx % self.params[\"print_logger\"] == 0 or batch_idx == len(train_loader) - 1:\n",
    "              print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f} (avg: {:.6f}) \\tsec/iter: {:.4f}'.format(\n",
    "                  epoch, n_samples, len(train_loader.dataset),\n",
    "                  100. * (batch_idx + 1) / len(train_loader), loss.item(), train_loss / n_samples, time_iter / (batch_idx + 1) ))\n",
    "      scheduler.step()\n",
    "      return total_time_iter / (len(train_loader) + 1)\n",
    "      \n",
    "\n",
    "    def test(self, test_loader, epoch):\n",
    "\n",
    "      print('Test model ...')\n",
    "\n",
    "      self.model.eval()\n",
    "      test_loss, correct, n_samples = 0, 0, 0\n",
    "\n",
    "      for batch_idx, data in enumerate(test_loader):\n",
    "          for i in range(len(data)):\n",
    "            data[i] = data[i].to(self.device)\n",
    "          \n",
    "          output = self.model(data)\n",
    "          loss = self.loss_fn(output, data[4], reduction='sum')\n",
    "          test_loss += loss.item()\n",
    "          n_samples += len(output)\n",
    "          pred = output.detach().cpu().max(1, keepdim=True)[1]\n",
    "\n",
    "          correct += pred.eq(data[4].detach().cpu().view_as(pred)).sum().item()\n",
    "\n",
    "      test_loss /= n_samples\n",
    "\n",
    "      acc = 100. * correct / n_samples\n",
    "\n",
    "      print('Test set (epoch {}): Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(epoch, \n",
    "                                                                                            test_loss, \n",
    "                                                                                            correct, \n",
    "                                                                                            n_samples, acc))\n",
    "      \n",
    "      return acc\n",
    "            \n",
    "              \n",
    "    def fit(self): \n",
    "        for fold_id in tqdm(range(self.params[\"n_folds\"]), desc=\"Folds\", position=0, leave=True):\n",
    "            loaders, optimizer, scheduler = self.loaders_train_test_setup()\n",
    "            total_time = 0\n",
    "\n",
    "            for epoch in tqdm(range(self.params[\"epochs\"]), desc=f\"Epochs (Fold {fold_id})\", position=1, leave=False):\n",
    "                total_time_iter = self.train(loaders, optimizer, scheduler, epoch)\n",
    "                total_time += total_time_iter\n",
    "                acc = self.test(loaders, epoch)\n",
    "\n",
    "            self.acc_folds.append(round(acc, 2))\n",
    "            self.time_folds.append(round(total_time / self.params[\"epochs\"], 2))\n",
    "          \n",
    "        print(self.acc_folds)\n",
    "        print('{}-fold cross validation avg acc (+- std): {} ({})'.format(\n",
    "            self.params[\"n_folds\"], statistics.mean(self.acc_folds), statistics.stdev(self.acc_folds)))\n",
    "        \n",
    "        result_list = [self.params[\"dataset\"], self.params[\"dataset\"]]\n",
    "        result_list.extend(str(acc_fold) for acc_fold in self.acc_folds)\n",
    "        result_list.extend([\n",
    "            statistics.mean(self.acc_folds),\n",
    "            statistics.stdev(self.acc_folds),\n",
    "            statistics.mean(self.time_folds)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "294\n",
      "89\n",
      "89\n"
     ]
    }
   ],
   "source": [
    "trainer = Train(params_list, dataaaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Folds:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N trainable parameters: 8578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Folds:   0%|          | 0/10 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9515, 89])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[2, 23323, 64]' is invalid for input of size 608960",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 156\u001b[0m, in \u001b[0;36mTrain.fit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    153\u001b[0m total_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m]), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpochs (Fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m, position\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m--> 156\u001b[0m     total_time_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m     total_time \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m total_time_iter\n\u001b[1;32m    158\u001b[0m     acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtest(loaders, epoch)\n",
      "Cell \u001b[0;32mIn[10], line 103\u001b[0m, in \u001b[0;36mTrain.train\u001b[0;34m(self, train_loader, optimizer, scheduler, epoch)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[1;32m    102\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 103\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(output, data[\u001b[38;5;241m4\u001b[39m])\n\u001b[1;32m    105\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/Documents/MVA/MVA S2/Graphical Models/MVA_Graphical_Models/my_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MVA/MVA S2/Graphical Models/MVA_Graphical_Models/my_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MVA/MVA S2/Graphical Models/MVA_Graphical_Models/src/models.py:37\u001b[0m, in \u001b[0;36mGCN.forward\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layer):\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m---> 37\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph_convolution_layers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_layer \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     40\u001b[0m         x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mdropout(x, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n",
      "File \u001b[0;32m~/Documents/MVA/MVA S2/Graphical Models/MVA_Graphical_Models/my_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/MVA/MVA S2/Graphical Models/MVA_Graphical_Models/my_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/MVA/MVA S2/Graphical Models/MVA_Graphical_Models/src/layer.py:34\u001b[0m, in \u001b[0;36mGraphConvolutionLayer.forward\u001b[0;34m(self, x, adj)\u001b[0m\n\u001b[1;32m     32\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     33\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmm(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n\u001b[0;32m---> 34\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43madj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbmm(adj, x)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[2, 23323, 64]' is invalid for input of size 608960"
     ]
    }
   ],
   "source": [
    "trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
